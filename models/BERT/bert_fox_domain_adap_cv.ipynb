{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vkr-bw_b1JQX"
   },
   "source": [
    "# Hate speech classification using fox news dataset\n",
    "\n",
    "Consists of: in-domain results and domain adaptation on movies dataset results\n",
    "\n",
    "The class labels depict the following:\n",
    "\n",
    "0: Normal speech, \n",
    "1: Hate speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To work with this, the following folder paths needs to be created in the directory of this notebook:\n",
    "\n",
    "classification_reports/   : This will contain all the classification reports generated by the model\n",
    "\n",
    "data/         : Contains fox_news.csv annotation file\n",
    "\n",
    "movies/       : contains all_movies.csv file\n",
    "\n",
    "movies/for_training/:    contains 6 movies used for cross validation training and testing\n",
    "\n",
    "training_checkpoints/in_domain/fox/cp_fox.ckpt  : for storing the weights of execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3cqyGp3sQg0h",
    "outputId": "69797d28-1df7-47ca-c19e-335bb1886d5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3YwUM7V6OewJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SI4UAcYcOpxc",
    "outputId": "84276a8f-4f5c-4f50-b3c0-f1241bbb05b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# initialize bert for 2 labels\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                        trainable=True, \n",
    "                                                        num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-V8R0MLrTyKA",
    "outputId": "7245e21c-46ef-43b8-efec-95326b23474b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97-sBX4P0Quk"
   },
   "source": [
    "Initialize checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RXneYXFPkTO7"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"training_checkpoints/in_domain/fox/cp_fox.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaTErsaJst7w"
   },
   "source": [
    "Read hate dataset and convert it into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kd0PtS8gQqaq"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/fox_news.csv\")\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "eeIgDpccr2MT",
    "outputId": "e44d1eb7-1a15-47e8-df27-090e0a3fec5a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merkel would never say NO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Expect more and more women to be asking .. \"wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Groping people in public wasn't already illega...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Merkel, possible the only person in charge who...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They know very well, no means NO They need to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label\n",
       "0                          Merkel would never say NO      1\n",
       "1  Expect more and more women to be asking .. \"wh...      1\n",
       "2  Groping people in public wasn't already illega...      0\n",
       "3  Merkel, possible the only person in charge who...      1\n",
       "4  They know very well, no means NO They need to ...      1"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['label'].replace({2:1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Jd4H6QsirNR0"
   },
   "outputs": [],
   "source": [
    "def get_dataset(df, seed, test_size):\n",
    "    return train_test_split(df, test_size=test_size, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_K1WDeHlrNR1"
   },
   "outputs": [],
   "source": [
    "train, test = get_dataset(df, 11, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKcwAZ0SIAmS",
    "outputId": "21d6b8c5-9926-49ea-a505-649282f06ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1210 entries, 941 to 1104\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   comment  1210 non-null   object\n",
      " 1   label    1210 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 28.4+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "K8-1iAh8rNR6"
   },
   "outputs": [],
   "source": [
    "train.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
    "test.columns = ['DATA_COLUMN', 'LABEL_COLUMN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "qdwNsE961nYm",
    "outputId": "6d34b08e-d306-4fc6-8ab2-b66c4941ad2e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA_COLUMN</th>\n",
       "      <th>LABEL_COLUMN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>It sure looks that way</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>Spoken like a true Wookie.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>Someone will dox the snarky Snarth. It's only ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>Oh my, you just blew LBJ's \"war against povert...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>I did make a statement. I admit that</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            DATA_COLUMN  LABEL_COLUMN\n",
       "941                              It sure looks that way             0\n",
       "569                          Spoken like a true Wookie.             1\n",
       "1496  Someone will dox the snarky Snarth. It's only ...             1\n",
       "1129  Oh my, you just blew LBJ's \"war against povert...             0\n",
       "259                I did make a statement. I admit that             0"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_cdRKItSW0Cz"
   },
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
    "  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "  \n",
    "  return train_InputExamples, validation_InputExamples\n",
    "\n",
    "  \n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_COLUMN = 'DATA_COLUMN'\n",
    "LABEL_COLUMN = 'LABEL_COLUMN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mOm1oM-HXaby",
    "outputId": "85e34fa2-710e-44d5-df47-89ab1f8eddda"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_data = train_data.batch(32)\n",
    "\n",
    "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_data = validation_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ujXS3VX8XjpE"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-6, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLWnbzCQmgu3",
    "outputId": "8e11b057-88a7-4e22-8cd6-95a84c6563be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f7104feed70>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f7104feed70>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f71201a1170> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function wrap at 0x7f71201a1170> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convertWARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "     38/Unknown - 84s 780ms/step - loss: 0.6529 - accuracy: 0.6446WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "38/38 [==============================] - 89s 923ms/step - loss: 0.6529 - accuracy: 0.6446 - val_loss: 0.5894 - val_accuracy: 0.7558\n",
      "\n",
      "Epoch 00001: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 2/17\n",
      "38/38 [==============================] - 34s 892ms/step - loss: 0.6089 - accuracy: 0.7058 - val_loss: 0.5671 - val_accuracy: 0.7558\n",
      "\n",
      "Epoch 00002: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 3/17\n",
      "38/38 [==============================] - 33s 871ms/step - loss: 0.5925 - accuracy: 0.7058 - val_loss: 0.5549 - val_accuracy: 0.7558\n",
      "\n",
      "Epoch 00003: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 4/17\n",
      "38/38 [==============================] - 33s 881ms/step - loss: 0.5767 - accuracy: 0.7099 - val_loss: 0.5424 - val_accuracy: 0.7525\n",
      "\n",
      "Epoch 00004: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 5/17\n",
      "38/38 [==============================] - 33s 874ms/step - loss: 0.5502 - accuracy: 0.7256 - val_loss: 0.5325 - val_accuracy: 0.7492\n",
      "\n",
      "Epoch 00005: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 6/17\n",
      "38/38 [==============================] - 33s 878ms/step - loss: 0.5252 - accuracy: 0.7479 - val_loss: 0.5225 - val_accuracy: 0.7525\n",
      "\n",
      "Epoch 00006: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 7/17\n",
      "38/38 [==============================] - 33s 875ms/step - loss: 0.4958 - accuracy: 0.7702 - val_loss: 0.5121 - val_accuracy: 0.7624\n",
      "\n",
      "Epoch 00007: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 8/17\n",
      "38/38 [==============================] - 33s 877ms/step - loss: 0.4572 - accuracy: 0.7967 - val_loss: 0.5068 - val_accuracy: 0.7657\n",
      "\n",
      "Epoch 00008: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 9/17\n",
      "38/38 [==============================] - 33s 877ms/step - loss: 0.4170 - accuracy: 0.8339 - val_loss: 0.5063 - val_accuracy: 0.7789\n",
      "\n",
      "Epoch 00009: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 10/17\n",
      "38/38 [==============================] - 33s 878ms/step - loss: 0.3742 - accuracy: 0.8521 - val_loss: 0.5011 - val_accuracy: 0.7822\n",
      "\n",
      "Epoch 00010: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 11/17\n",
      "38/38 [==============================] - 33s 873ms/step - loss: 0.3316 - accuracy: 0.8793 - val_loss: 0.5140 - val_accuracy: 0.7855\n",
      "\n",
      "Epoch 00011: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 12/17\n",
      "38/38 [==============================] - 33s 878ms/step - loss: 0.2863 - accuracy: 0.9074 - val_loss: 0.5274 - val_accuracy: 0.7822\n",
      "\n",
      "Epoch 00012: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 13/17\n",
      "38/38 [==============================] - 33s 877ms/step - loss: 0.2395 - accuracy: 0.9264 - val_loss: 0.5337 - val_accuracy: 0.7888\n",
      "\n",
      "Epoch 00013: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 14/17\n",
      "38/38 [==============================] - 33s 877ms/step - loss: 0.2102 - accuracy: 0.9471 - val_loss: 0.5542 - val_accuracy: 0.7789\n",
      "\n",
      "Epoch 00014: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 15/17\n",
      "38/38 [==============================] - 33s 877ms/step - loss: 0.1732 - accuracy: 0.9603 - val_loss: 0.5759 - val_accuracy: 0.7789\n",
      "\n",
      "Epoch 00015: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 16/17\n",
      "38/38 [==============================] - 33s 877ms/step - loss: 0.1431 - accuracy: 0.9669 - val_loss: 0.5741 - val_accuracy: 0.7789\n",
      "\n",
      "Epoch 00016: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n",
      "Epoch 17/17\n",
      "38/38 [==============================] - 33s 877ms/step - loss: 0.1258 - accuracy: 0.9719 - val_loss: 0.6194 - val_accuracy: 0.7822\n",
      "\n",
      "Epoch 00017: saving model to training_checkpoints/in_domain/fox/cp_fox.ckpt\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_data, epochs=17, validation_data=validation_data, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJe6wC0HrlX6",
    "outputId": "70637ca2-195c-44b7-c1c8-a4ee37a024f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaYIQvV90d7Y"
   },
   "source": [
    "#### In-domain classification report for fox news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdjt4GhlqGNB",
    "outputId": "74a66a12-a8ac-478c-c7b9-1e5fafa73cd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86       229\n",
      "           1       0.57      0.46      0.51        74\n",
      "\n",
      "    accuracy                           0.78       303\n",
      "   macro avg       0.70      0.67      0.68       303\n",
      "weighted avg       0.77      0.78      0.77       303\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['LABEL_COLUMN'],np.argmax(preds[0],axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "VZFSpUcQvNz2"
   },
   "outputs": [],
   "source": [
    "cr = classification_report(test['LABEL_COLUMN'],np.argmax(preds[0],axis=1),output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "JJLTRyWYvT_N"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cr).transpose().to_csv('classification_reports/classification_bert_fox_indomain.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLr2zMyJ1qKD"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Domain Adaptation, predicting on movies with the fox trained model on 2 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "f-qYIKfU0Hbx"
   },
   "outputs": [],
   "source": [
    "def convert_data_to_examples_valid(data, DATA_COLUMN, LABEL_COLUMN): \n",
    "  inputExamples = data.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "  \n",
    "  return inputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bWO0Gd0kJcjc"
   },
   "outputs": [],
   "source": [
    "df_movies = pd.read_csv('movies/all_movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "kPCquRoxzgbg",
    "outputId": "4068447a-0166-444e-f1e9-25a45d460820"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>LABEL_COLUMN</th>\n",
       "      <th>DATA_COLUMN</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AmericanHistoryX(1998)_1</td>\n",
       "      <td>1566624979</td>\n",
       "      <td>0</td>\n",
       "      <td>Derek.</td>\n",
       "      <td>AmerricanHistoryX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AmericanHistoryX(1998)_2</td>\n",
       "      <td>1566624979</td>\n",
       "      <td>1</td>\n",
       "      <td>What the fuck are you thinking?</td>\n",
       "      <td>AmerricanHistoryX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AmericanHistoryX(1998)_3</td>\n",
       "      <td>1566624979</td>\n",
       "      <td>0</td>\n",
       "      <td>There's a black guy outside breaking into your...</td>\n",
       "      <td>AmerricanHistoryX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AmericanHistoryX(1998)_4</td>\n",
       "      <td>1566624979</td>\n",
       "      <td>0</td>\n",
       "      <td>How long has he been there?</td>\n",
       "      <td>AmerricanHistoryX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AmericanHistoryX(1998)_5</td>\n",
       "      <td>1566624979</td>\n",
       "      <td>0</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>AmerricanHistoryX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  movie_id  ...  Unnamed: 6  Unnamed: 7\n",
       "0           0  AmericanHistoryX(1998)_1  ...         NaN         NaN\n",
       "1           1  AmericanHistoryX(1998)_2  ...         NaN         NaN\n",
       "2           2  AmericanHistoryX(1998)_3  ...         NaN         NaN\n",
       "3           3  AmericanHistoryX(1998)_4  ...         NaN         NaN\n",
       "4           4  AmericanHistoryX(1998)_5  ...         NaN         NaN\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies = df_movies.rename(columns={\"text\": \"DATA_COLUMN\", \"majority_answer\": \"LABEL_COLUMN\"})\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "UA5X5W5C1aiB"
   },
   "outputs": [],
   "source": [
    "# using 2 labels in movies\n",
    "df_movies_2col = df_movies.copy()\n",
    "df_movies_2col['LABEL_COLUMN'] = df_movies_2col['LABEL_COLUMN'].replace(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_xhlMVV4KbJ",
    "outputId": "660e45ca-70fc-4a32-8794-f116d3c05fef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "movie2_InputExamples = convert_data_to_examples_valid(df_movies_2col, DATA_COLUMN, LABEL_COLUMN)\n",
    "movie2_data = convert_examples_to_tf_dataset(list(movie2_InputExamples), tokenizer)\n",
    "movie2_data = movie2_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Oe5HbKwH2YQ2"
   },
   "outputs": [],
   "source": [
    "preds_movie = model.predict(movie2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "GdK6JxMH4TUb"
   },
   "outputs": [],
   "source": [
    "cr_movies = classification_report(df_movies_2col['LABEL_COLUMN'], np.argmax(preds_movie[0], axis=1), output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "njZcEAEE4zIW"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cr_movies).transpose().to_csv('classification_reports/bert_fox_domain_adap_movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ColJ7r4G0-ic"
   },
   "source": [
    "#### Domain adaptation classification report from fox news on the movies dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "XAsCNx8NTsVB",
    "outputId": "8cb72f09-4241-4fff-dc81-73038bc1b71a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.885151</td>\n",
       "      <td>0.896051</td>\n",
       "      <td>0.890567</td>\n",
       "      <td>9014.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.400512</td>\n",
       "      <td>0.373955</td>\n",
       "      <td>0.386778</td>\n",
       "      <td>1674.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.814278</td>\n",
       "      <td>0.814278</td>\n",
       "      <td>0.814278</td>\n",
       "      <td>0.814278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.642831</td>\n",
       "      <td>0.635003</td>\n",
       "      <td>0.638673</td>\n",
       "      <td>10688.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.809244</td>\n",
       "      <td>0.814278</td>\n",
       "      <td>0.811662</td>\n",
       "      <td>10688.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score       support\n",
       "0              0.885151  0.896051  0.890567   9014.000000\n",
       "1              0.400512  0.373955  0.386778   1674.000000\n",
       "accuracy       0.814278  0.814278  0.814278      0.814278\n",
       "macro avg      0.642831  0.635003  0.638673  10688.000000\n",
       "weighted avg   0.809244  0.814278  0.811662  10688.000000"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cr_movies).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "M12KEQe3zL5r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2J1kcn3M5nEi"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Cross validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccJl8pYO5yI-"
   },
   "source": [
    "#### 6-fold cross validation on movies by fine tuning on above fox dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "VZxJz_OBEBAO"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "xRSnyXKl5xol"
   },
   "outputs": [],
   "source": [
    "def convert_data_to_examples_cv(train, DATA_COLUMN, LABEL_COLUMN):\n",
    "    train_InputExamples = train.apply(\n",
    "        lambda x: InputExample(guid=None,  # Globally unique ID for bookkeeping, unused in this case\n",
    "                               text_a=x[DATA_COLUMN],\n",
    "                               text_b=None,\n",
    "                               label=x[LABEL_COLUMN]), axis=1)\n",
    "\n",
    "    return train_InputExamples\n",
    "\n",
    "\n",
    "def convert_examples_to_tf_dataset_cv(examples, tokenizer, max_length=128):\n",
    "    features = []  # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,  # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True,  # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "                                                     input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def train_bert(df_train, df_test, load_weights = False):\n",
    "    model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                            trainable=True,\n",
    "                                                            num_labels=2)\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    if load_weights:\n",
    "\t    model.load_weights('training_checkpoints/in_domain/fox/cp_fox.ckpt')\n",
    "    train = df_train[['text', 'majority_answer']]\n",
    "    train.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
    "\n",
    "    test = df_test[['text', 'majority_answer']]\n",
    "    test.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
    "\n",
    "    DATA_COLUMN = 'DATA_COLUMN'\n",
    "    LABEL_COLUMN = 'LABEL_COLUMN'\n",
    "\n",
    "    train_InputExamples = convert_data_to_examples_cv(train, DATA_COLUMN, LABEL_COLUMN)\n",
    "    test_InputExamples = convert_data_to_examples_cv(test, DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "    train_data = convert_examples_to_tf_dataset_cv(list(train_InputExamples), tokenizer)\n",
    "    train_data = train_data.batch(32)\n",
    "\n",
    "    # compile and fit\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-6, epsilon=1e-08, clipnorm=1.0),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "    model.fit(train_data, epochs=6)\n",
    "\n",
    "    test_data = convert_examples_to_tf_dataset_cv(list(test_InputExamples), tokenizer)\n",
    "    test_data = test_data.batch(32)\n",
    "\n",
    "    print('predicting')\n",
    "    preds = model.predict(test_data)\n",
    "\n",
    "    # classification\n",
    "    return classification_report(pd.DataFrame(test['LABEL_COLUMN']), np.argmax(preds[0], axis=1), output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "DWsFTqlHlgv3"
   },
   "outputs": [],
   "source": [
    "def load_movies_to_df(path):\n",
    "    df_movies = []\n",
    "\n",
    "    for filename in glob.glob(path + '*.csv'):\n",
    "        df_movies.append(pd.read_csv(filename))\n",
    "\n",
    "    return df_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Xz1rCrpvB2RA"
   },
   "outputs": [],
   "source": [
    "df_movies = load_movies_to_df('movies/for_training/')\n",
    "classification_reports = []\n",
    "df_main = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpchIXr0lciQ",
    "outputId": "0204a8ac-4be9-4e77-f451-92e112ac5b8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulp_Fiction,AmerricanHistoryX,TheWolfofWallStreet,Django_Unchained,South_Park\n",
      "BlacKkKlansman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "283/283 [==============================] - 246s 799ms/step - loss: 0.1848 - accuracy: 0.9340\n",
      "Epoch 2/6\n",
      "283/283 [==============================] - 226s 798ms/step - loss: 0.1135 - accuracy: 0.9631\n",
      "Epoch 3/6\n",
      "283/283 [==============================] - 226s 797ms/step - loss: 0.0955 - accuracy: 0.9707\n",
      "Epoch 4/6\n",
      "283/283 [==============================] - 226s 797ms/step - loss: 0.0825 - accuracy: 0.9750\n",
      "Epoch 5/6\n",
      "283/283 [==============================] - 226s 797ms/step - loss: 0.0719 - accuracy: 0.9798\n",
      "Epoch 6/6\n",
      "283/283 [==============================] - 226s 798ms/step - loss: 0.0613 - accuracy: 0.9818\n",
      "predicting\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Train movies:  Pulp_Fiction,AmerricanHistoryX,TheWolfofWallStreet,Django_Unchained,South_Park\n",
      "Test movie:  BlacKkKlansman\n",
      "Classification report: \n",
      " {'0': {'precision': 0.969632768361582, 'recall': 0.939124487004104, 'f1-score': 0.9541348158443363, 'support': 1462}, '1': {'precision': 0.611353711790393, 'recall': 0.7650273224043715, 'f1-score': 0.6796116504854368, 'support': 183}, 'accuracy': 0.9197568389057751, 'macro avg': {'precision': 0.7904932400759874, 'recall': 0.8520759047042378, 'f1-score': 0.8168732331648866, 'support': 1645}, 'weighted avg': {'precision': 0.9297755845606535, 'recall': 0.9197568389057751, 'f1-score': 0.9235951567193037, 'support': 1645}}\n",
      "------------------------------------------------\n",
      "BlacKkKlansman,AmerricanHistoryX,TheWolfofWallStreet,Django_Unchained,South_Park\n",
      "Pulp_Fiction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "284/284 [==============================] - 247s 798ms/step - loss: 0.1888 - accuracy: 0.9344\n",
      "Epoch 2/6\n",
      "284/284 [==============================] - 226s 796ms/step - loss: 0.1146 - accuracy: 0.9646\n",
      "Epoch 3/6\n",
      "284/284 [==============================] - 226s 797ms/step - loss: 0.0931 - accuracy: 0.9698\n",
      "Epoch 4/6\n",
      "284/284 [==============================] - 226s 797ms/step - loss: 0.0811 - accuracy: 0.9746\n",
      "Epoch 5/6\n",
      "284/284 [==============================] - 226s 796ms/step - loss: 0.0657 - accuracy: 0.9798\n",
      "Epoch 6/6\n",
      "284/284 [==============================] - 226s 796ms/step - loss: 0.0550 - accuracy: 0.9831\n",
      "predicting\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Train movies:  BlacKkKlansman,AmerricanHistoryX,TheWolfofWallStreet,Django_Unchained,South_Park\n",
      "Test movie:  Pulp_Fiction\n",
      "Classification report: \n",
      " {'0': {'precision': 0.964951528709918, 'recall': 0.9707426856714179, 'f1-score': 0.967838444278235, 'support': 1333}, '1': {'precision': 0.8612099644128114, 'recall': 0.8373702422145328, 'f1-score': 0.8491228070175437, 'support': 289}, 'accuracy': 0.9469790382244143, 'macro avg': {'precision': 0.9130807465613646, 'recall': 0.9040564639429753, 'f1-score': 0.9084806256478893, 'support': 1622}, 'weighted avg': {'precision': 0.9464673658974249, 'recall': 0.9469790382244143, 'f1-score': 0.9466862746306766, 'support': 1622}}\n",
      "------------------------------------------------\n",
      "BlacKkKlansman,Pulp_Fiction,TheWolfofWallStreet,Django_Unchained,South_Park\n",
      "AmerricanHistoryX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "286/286 [==============================] - 250s 798ms/step - loss: 0.1917 - accuracy: 0.9355\n",
      "Epoch 2/6\n",
      "286/286 [==============================] - 228s 796ms/step - loss: 0.1174 - accuracy: 0.9625\n",
      "Epoch 3/6\n",
      "286/286 [==============================] - 228s 797ms/step - loss: 0.0918 - accuracy: 0.9706\n",
      "Epoch 4/6\n",
      "286/286 [==============================] - 228s 797ms/step - loss: 0.0787 - accuracy: 0.9753\n",
      "Epoch 5/6\n",
      "286/286 [==============================] - 228s 797ms/step - loss: 0.0665 - accuracy: 0.9804\n",
      "Epoch 6/6\n",
      "286/286 [==============================] - 228s 797ms/step - loss: 0.0585 - accuracy: 0.9840\n",
      "predicting\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Train movies:  BlacKkKlansman,Pulp_Fiction,TheWolfofWallStreet,Django_Unchained,South_Park\n",
      "Test movie:  AmerricanHistoryX\n",
      "Classification report: \n",
      " {'0': {'precision': 0.9573672400897532, 'recall': 0.9815950920245399, 'f1-score': 0.96932979931844, 'support': 1304}, '1': {'precision': 0.8947368421052632, 'recall': 0.7816091954022989, 'f1-score': 0.8343558282208589, 'support': 261}, 'accuracy': 0.9482428115015974, 'macro avg': {'precision': 0.9260520410975082, 'recall': 0.8816021437134194, 'f1-score': 0.9018428137696495, 'support': 1565}, 'weighted avg': {'precision': 0.9469221705217329, 'recall': 0.9482428115015974, 'f1-score': 0.9468197632440193, 'support': 1565}}\n",
      "------------------------------------------------\n",
      "BlacKkKlansman,Pulp_Fiction,AmerricanHistoryX,Django_Unchained,South_Park\n",
      "TheWolfofWallStreet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "239/239 [==============================] - 211s 799ms/step - loss: 0.2170 - accuracy: 0.9214\n",
      "Epoch 2/6\n",
      "239/239 [==============================] - 190s 797ms/step - loss: 0.1362 - accuracy: 0.9583\n",
      "Epoch 3/6\n",
      "239/239 [==============================] - 191s 797ms/step - loss: 0.1155 - accuracy: 0.9643\n",
      "Epoch 4/6\n",
      "239/239 [==============================] - 191s 797ms/step - loss: 0.1010 - accuracy: 0.9696\n",
      "Epoch 5/6\n",
      "239/239 [==============================] - 191s 798ms/step - loss: 0.0869 - accuracy: 0.9739\n",
      "Epoch 6/6\n",
      "239/239 [==============================] - 190s 797ms/step - loss: 0.0773 - accuracy: 0.9777\n",
      "predicting\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Train movies:  BlacKkKlansman,Pulp_Fiction,AmerricanHistoryX,Django_Unchained,South_Park\n",
      "Test movie:  TheWolfofWallStreet\n",
      "Classification report: \n",
      " {'0': {'precision': 0.9810024252223121, 'recall': 0.9810024252223121, 'f1-score': 0.9810024252223121, 'support': 2474}, '1': {'precision': 0.9202037351443124, 'recall': 0.9202037351443124, 'f1-score': 0.9202037351443124, 'support': 589}, 'accuracy': 0.9693111328762651, 'macro avg': {'precision': 0.9506030801833123, 'recall': 0.9506030801833123, 'f1-score': 0.9506030801833123, 'support': 3063}, 'weighted avg': {'precision': 0.9693111328762651, 'recall': 0.9693111328762651, 'f1-score': 0.9693111328762651, 'support': 3063}}\n",
      "------------------------------------------------\n",
      "BlacKkKlansman,Pulp_Fiction,AmerricanHistoryX,TheWolfofWallStreet,South_Park\n",
      "Django_Unchained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "280/280 [==============================] - 244s 799ms/step - loss: 0.2104 - accuracy: 0.9281\n",
      "Epoch 2/6\n",
      "280/280 [==============================] - 223s 798ms/step - loss: 0.1279 - accuracy: 0.9571\n",
      "Epoch 3/6\n",
      "280/280 [==============================] - 223s 797ms/step - loss: 0.1027 - accuracy: 0.9668\n",
      "Epoch 4/6\n",
      "280/280 [==============================] - 223s 798ms/step - loss: 0.0871 - accuracy: 0.9720\n",
      "Epoch 5/6\n",
      "280/280 [==============================] - 224s 798ms/step - loss: 0.0766 - accuracy: 0.9774\n",
      "Epoch 6/6\n",
      "280/280 [==============================] - 224s 798ms/step - loss: 0.0657 - accuracy: 0.9807\n",
      "predicting\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Train movies:  BlacKkKlansman,Pulp_Fiction,AmerricanHistoryX,TheWolfofWallStreet,South_Park\n",
      "Test movie:  Django_Unchained\n",
      "Classification report: \n",
      " {'0': {'precision': 0.9825806451612903, 'recall': 0.9819471308833011, 'f1-score': 0.9822637858755241, 'support': 1551}, '1': {'precision': 0.8578680203045685, 'recall': 0.8622448979591837, 'f1-score': 0.8600508905852418, 'support': 196}, 'accuracy': 0.9685174585002863, 'macro avg': {'precision': 0.9202243327329294, 'recall': 0.9220960144212424, 'f1-score': 0.9211573382303829, 'support': 1747}, 'weighted avg': {'precision': 0.9685888452346061, 'recall': 0.9685174585002863, 'f1-score': 0.9685524364325387, 'support': 1747}}\n",
      "------------------------------------------------\n",
      "BlacKkKlansman,Pulp_Fiction,AmerricanHistoryX,TheWolfofWallStreet,Django_Unchained\n",
      "South_Park\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "302/302 [==============================] - 262s 799ms/step - loss: 0.1957 - accuracy: 0.9346\n",
      "Epoch 2/6\n",
      "302/302 [==============================] - 241s 797ms/step - loss: 0.1145 - accuracy: 0.9631\n",
      "Epoch 3/6\n",
      "302/302 [==============================] - 241s 797ms/step - loss: 0.0949 - accuracy: 0.9699\n",
      "Epoch 4/6\n",
      "302/302 [==============================] - 241s 797ms/step - loss: 0.0816 - accuracy: 0.9752\n",
      "Epoch 5/6\n",
      "302/302 [==============================] - 241s 797ms/step - loss: 0.0684 - accuracy: 0.9796\n",
      "Epoch 6/6\n",
      "302/302 [==============================] - 241s 797ms/step - loss: 0.0580 - accuracy: 0.9833\n",
      "predicting\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Train movies:  BlacKkKlansman,Pulp_Fiction,AmerricanHistoryX,TheWolfofWallStreet,Django_Unchained\n",
      "Test movie:  South_Park\n",
      "Classification report: \n",
      " {'0': {'precision': 0.9455930359085963, 'recall': 0.9764044943820225, 'f1-score': 0.9607517965726922, 'support': 890}, '1': {'precision': 0.8346456692913385, 'recall': 0.6794871794871795, 'f1-score': 0.7491166077738516, 'support': 156}, 'accuracy': 0.9321223709369025, 'macro avg': {'precision': 0.8901193525999674, 'recall': 0.827945836934601, 'f1-score': 0.8549342021732719, 'support': 1046}, 'weighted avg': {'precision': 0.9290463923213188, 'recall': 0.9321223709369025, 'f1-score': 0.9291886135395955, 'support': 1046}}\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# perform cross folding\n",
    "for i in range(len(df_movies)):\n",
    "    df_train = pd.concat(df_movies[0:i] + df_movies[i + 1:])\n",
    "    df_test = df_movies[i]\n",
    "\n",
    "    df_train['majority_answer'] = df_train['majority_answer'].replace({2:1})\n",
    "    df_test['majority_answer'] = df_test['majority_answer'].replace({2:1})\n",
    "\n",
    "    train_movies = df_train['movie_name'].unique()\n",
    "    test_movie = df_test['movie_name'].unique()\n",
    "    print(','.join(train_movies))\n",
    "    print(test_movie[0])\n",
    "    report = train_bert(df_train, df_test, True)\n",
    "    classification_reports.append(report)\n",
    "    \n",
    "    print('Train movies: ', str(','.join(train_movies)))\n",
    "    print('Test movie: ', str(test_movie[0]))\n",
    "    print('Classification report: \\n', classification_reports[i])\n",
    "    print('------------------------------------------------')\n",
    "\n",
    "    df_cr = pd.DataFrame(classification_reports[i]).transpose()\n",
    "    df_cr['movie_train'] =  str(','.join(train_movies))\n",
    "    df_cr['movie_test'] = str(test_movie[0])\n",
    "    df_cr.to_csv('classification_reports/'+'bert_fox_cv_finetune_testmovie_'+str(test_movie[0])+'.csv')\n",
    "    df_main = df_main.append(df_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "LyfPoHfVB63k"
   },
   "outputs": [],
   "source": [
    "df_main.to_csv('classification_reports/bert_crossvalid_finetune_fox.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9c-yeiBHfGG",
    "outputId": "5f777731-fbe9-499a-ab95-2bc08f392e91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision  ...           movie_test\n",
      "0              0.969633  ...       BlacKkKlansman\n",
      "1              0.611354  ...       BlacKkKlansman\n",
      "accuracy       0.919757  ...       BlacKkKlansman\n",
      "macro avg      0.790493  ...       BlacKkKlansman\n",
      "weighted avg   0.929776  ...       BlacKkKlansman\n",
      "0              0.964952  ...         Pulp_Fiction\n",
      "1              0.861210  ...         Pulp_Fiction\n",
      "accuracy       0.946979  ...         Pulp_Fiction\n",
      "macro avg      0.913081  ...         Pulp_Fiction\n",
      "weighted avg   0.946467  ...         Pulp_Fiction\n",
      "0              0.957367  ...    AmerricanHistoryX\n",
      "1              0.894737  ...    AmerricanHistoryX\n",
      "accuracy       0.948243  ...    AmerricanHistoryX\n",
      "macro avg      0.926052  ...    AmerricanHistoryX\n",
      "weighted avg   0.946922  ...    AmerricanHistoryX\n",
      "0              0.981002  ...  TheWolfofWallStreet\n",
      "1              0.920204  ...  TheWolfofWallStreet\n",
      "accuracy       0.969311  ...  TheWolfofWallStreet\n",
      "macro avg      0.950603  ...  TheWolfofWallStreet\n",
      "weighted avg   0.969311  ...  TheWolfofWallStreet\n",
      "0              0.982581  ...     Django_Unchained\n",
      "1              0.857868  ...     Django_Unchained\n",
      "accuracy       0.968517  ...     Django_Unchained\n",
      "macro avg      0.920224  ...     Django_Unchained\n",
      "weighted avg   0.968589  ...     Django_Unchained\n",
      "0              0.945593  ...           South_Park\n",
      "1              0.834646  ...           South_Park\n",
      "accuracy       0.932122  ...           South_Park\n",
      "macro avg      0.890119  ...           South_Park\n",
      "weighted avg   0.929046  ...           South_Park\n",
      "\n",
      "[30 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iD3D9odZB7de",
    "outputId": "8c1c92f0-5ca0-4877-ea36-bb8172573fe1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classification_reports[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "68L-CNl6GGg8",
    "outputId": "f62be9ae-54e5-48de-9dab-34cd1398f0bd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>movie_train</th>\n",
       "      <th>movie_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.969633</td>\n",
       "      <td>0.939124</td>\n",
       "      <td>0.954135</td>\n",
       "      <td>1462.000000</td>\n",
       "      <td>Pulp_Fiction,AmerricanHistoryX,TheWolfofWallSt...</td>\n",
       "      <td>BlacKkKlansman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.611354</td>\n",
       "      <td>0.765027</td>\n",
       "      <td>0.679612</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>Pulp_Fiction,AmerricanHistoryX,TheWolfofWallSt...</td>\n",
       "      <td>BlacKkKlansman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.919757</td>\n",
       "      <td>0.919757</td>\n",
       "      <td>0.919757</td>\n",
       "      <td>0.919757</td>\n",
       "      <td>Pulp_Fiction,AmerricanHistoryX,TheWolfofWallSt...</td>\n",
       "      <td>BlacKkKlansman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.790493</td>\n",
       "      <td>0.852076</td>\n",
       "      <td>0.816873</td>\n",
       "      <td>1645.000000</td>\n",
       "      <td>Pulp_Fiction,AmerricanHistoryX,TheWolfofWallSt...</td>\n",
       "      <td>BlacKkKlansman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.929776</td>\n",
       "      <td>0.919757</td>\n",
       "      <td>0.923595</td>\n",
       "      <td>1645.000000</td>\n",
       "      <td>Pulp_Fiction,AmerricanHistoryX,TheWolfofWallSt...</td>\n",
       "      <td>BlacKkKlansman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  ...      movie_test\n",
       "0              0.969633  ...  BlacKkKlansman\n",
       "1              0.611354  ...  BlacKkKlansman\n",
       "accuracy       0.919757  ...  BlacKkKlansman\n",
       "macro avg      0.790493  ...  BlacKkKlansman\n",
       "weighted avg   0.929776  ...  BlacKkKlansman\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "NRGObHolGHHF"
   },
   "outputs": [],
   "source": [
    "def get_precision_recall_f1(category, result_df):\n",
    "    precision = result_df[result_df.label==category].precision.mean()\n",
    "    recall = result_df[result_df.label==category].recall.mean()\n",
    "    f1 = result_df[result_df.label==category]['f1-score'].mean()\n",
    "    \n",
    "    return {'label': category, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "vj_ZEFYdRdF4"
   },
   "outputs": [],
   "source": [
    "df_cv= pd.read_csv('classification_reports/bert_crossvalid_finetune_fox.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "SnXkEFJlmfSU",
    "outputId": "04dd443b-77c3-497d-c7eb-2078de16887e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>movie_train</th>\n",
       "      <th>movie_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.969633</td>\n",
       "      <td>0.939124</td>\n",
       "      <td>0.954135</td>\n",
       "      <td>1462.000000</td>\n",
       "      <td>Pulp_Fiction,AmerricanHistoryX,TheWolfofWallSt...</td>\n",
       "      <td>BlacKkKlansman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.611354</td>\n",
       "      <td>0.765027</td>\n",
       "      <td>0.679612</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>Pulp_Fiction,AmerricanHistoryX,TheWolfofWallSt...</td>\n",
       "      <td>BlacKkKlansman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.919757</td>\n",
       "      <td>0.919757</td>\n",
       "      <td>0.919757</td>\n",
       "      <td>0.919757</td>\n",
       "      <td>Pulp_Fiction,AmerricanHistoryX,TheWolfofWallSt...</td>\n",
       "      <td>BlacKkKlansman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.790493</td>\n",
       "      <td>0.852076</td>\n",
       "      <td>0.816873</td>\n",
       "      <td>1645.000000</td>\n",
       "      <td>Pulp_Fiction,AmerricanHistoryX,TheWolfofWallSt...</td>\n",
       "      <td>BlacKkKlansman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.929776</td>\n",
       "      <td>0.919757</td>\n",
       "      <td>0.923595</td>\n",
       "      <td>1645.000000</td>\n",
       "      <td>Pulp_Fiction,AmerricanHistoryX,TheWolfofWallSt...</td>\n",
       "      <td>BlacKkKlansman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label  ...      movie_test\n",
       "0             0  ...  BlacKkKlansman\n",
       "1             1  ...  BlacKkKlansman\n",
       "2      accuracy  ...  BlacKkKlansman\n",
       "3     macro avg  ...  BlacKkKlansman\n",
       "4  weighted avg  ...  BlacKkKlansman\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv = df_cv.rename(columns={'Unnamed: 0': 'label', 'b': 'Y'})\n",
    "df_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "42LOVWQeRm6P"
   },
   "outputs": [],
   "source": [
    "normal_dict = get_precision_recall_f1('0', df_cv)\n",
    "offensive_dict = get_precision_recall_f1('1',df_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregated results of all 6 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "9xnoFFrOmgu1",
    "outputId": "5494520e-f902-4305-ab7f-5b9852c2c335"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.966855</td>\n",
       "      <td>0.971803</td>\n",
       "      <td>0.96922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.830003</td>\n",
       "      <td>0.807657</td>\n",
       "      <td>0.81541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  precision    recall       f1\n",
       "0     0   0.966855  0.971803  0.96922\n",
       "1     1   0.830003  0.807657  0.81541"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = pd.DataFrame([normal_dict, offensive_dict])\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLHfd4BFnloL",
    "outputId": "24cba12d-0e82-49b6-f96e-13f0a85311a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.969632768361582, 'recall': 0.939124487004104, 'f1-score': 0.9541348158443363, 'support': 1462}, '1': {'precision': 0.611353711790393, 'recall': 0.7650273224043715, 'f1-score': 0.6796116504854368, 'support': 183}, 'accuracy': 0.9197568389057751, 'macro avg': {'precision': 0.7904932400759874, 'recall': 0.8520759047042378, 'f1-score': 0.8168732331648866, 'support': 1645}, 'weighted avg': {'precision': 0.9297755845606535, 'recall': 0.9197568389057751, 'f1-score': 0.9235951567193037, 'support': 1645}}\n",
      "{'0': {'precision': 0.964951528709918, 'recall': 0.9707426856714179, 'f1-score': 0.967838444278235, 'support': 1333}, '1': {'precision': 0.8612099644128114, 'recall': 0.8373702422145328, 'f1-score': 0.8491228070175437, 'support': 289}, 'accuracy': 0.9469790382244143, 'macro avg': {'precision': 0.9130807465613646, 'recall': 0.9040564639429753, 'f1-score': 0.9084806256478893, 'support': 1622}, 'weighted avg': {'precision': 0.9464673658974249, 'recall': 0.9469790382244143, 'f1-score': 0.9466862746306766, 'support': 1622}}\n",
      "{'0': {'precision': 0.9573672400897532, 'recall': 0.9815950920245399, 'f1-score': 0.96932979931844, 'support': 1304}, '1': {'precision': 0.8947368421052632, 'recall': 0.7816091954022989, 'f1-score': 0.8343558282208589, 'support': 261}, 'accuracy': 0.9482428115015974, 'macro avg': {'precision': 0.9260520410975082, 'recall': 0.8816021437134194, 'f1-score': 0.9018428137696495, 'support': 1565}, 'weighted avg': {'precision': 0.9469221705217329, 'recall': 0.9482428115015974, 'f1-score': 0.9468197632440193, 'support': 1565}}\n",
      "{'0': {'precision': 0.9810024252223121, 'recall': 0.9810024252223121, 'f1-score': 0.9810024252223121, 'support': 2474}, '1': {'precision': 0.9202037351443124, 'recall': 0.9202037351443124, 'f1-score': 0.9202037351443124, 'support': 589}, 'accuracy': 0.9693111328762651, 'macro avg': {'precision': 0.9506030801833123, 'recall': 0.9506030801833123, 'f1-score': 0.9506030801833123, 'support': 3063}, 'weighted avg': {'precision': 0.9693111328762651, 'recall': 0.9693111328762651, 'f1-score': 0.9693111328762651, 'support': 3063}}\n",
      "{'0': {'precision': 0.9825806451612903, 'recall': 0.9819471308833011, 'f1-score': 0.9822637858755241, 'support': 1551}, '1': {'precision': 0.8578680203045685, 'recall': 0.8622448979591837, 'f1-score': 0.8600508905852418, 'support': 196}, 'accuracy': 0.9685174585002863, 'macro avg': {'precision': 0.9202243327329294, 'recall': 0.9220960144212424, 'f1-score': 0.9211573382303829, 'support': 1747}, 'weighted avg': {'precision': 0.9685888452346061, 'recall': 0.9685174585002863, 'f1-score': 0.9685524364325387, 'support': 1747}}\n",
      "{'0': {'precision': 0.9455930359085963, 'recall': 0.9764044943820225, 'f1-score': 0.9607517965726922, 'support': 890}, '1': {'precision': 0.8346456692913385, 'recall': 0.6794871794871795, 'f1-score': 0.7491166077738516, 'support': 156}, 'accuracy': 0.9321223709369025, 'macro avg': {'precision': 0.8901193525999674, 'recall': 0.827945836934601, 'f1-score': 0.8549342021732719, 'support': 1046}, 'weighted avg': {'precision': 0.9290463923213188, 'recall': 0.9321223709369025, 'f1-score': 0.9291886135395955, 'support': 1046}}\n"
     ]
    }
   ],
   "source": [
    "for cr in classification_reports:\n",
    "  print(cr)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bert_fox_domain_adap_cv.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
